# Text-to-Speech Papers
List of papers about TTS / Список статей о TTS

## Core
0. [Tacotron: Towards End-to-End Speech Synthesis](https://arxiv.org/pdf/1703.10135.pdf)
1. [Learning Phrase Representations Using RNN Encoder–Decoder for Statistical Machine Translation](https://arxiv.org/pdf/1406.1078v3.pdf)
2. [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215.pdf)
3. [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf) - Attention in RNNs
4. [Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks](http://papers.nips.cc/paper/5956-scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks.pdf)
5. [Fully Character-Level Neural Machine Translation without Explicit Segmentation](https://arxiv.org/pdf/1610.03017.pdf) - CBHG
6. [A Study of the Recurrent Neural Network Encoder-Decoder for Large Vocabulary Speech Recognition](http://homepages.inf.ed.ac.uk/srenals/ll-rnn-is15.pdf)
7. [Generating Sequences With Recurrent Neural Networks](https://arxiv.org/pdf/1308.0850)
8. [End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First Results](https://arxiv.org/pdf/1412.1602.pdf)
9. [Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism](https://arxiv.org/pdf/1601.01073.pdf)
10. [Grammar as a Foreign Language](https://arxiv.org/pdf/1412.7449.pdf) - Attention
11. [Long Short-Term Memory-Networks for Machine Reading](https://arxiv.org/pdf/1601.06733.pdf)
## Future improvements
0. [Improving Speech Recognition by Revising Gated Recurrent Units](https://arxiv.org/pdf/1710.00641.pdf) - GRU simplifying (less training time, better results)
1. [Layer Normalization](https://arxiv.org/pdf/1607.06450.pdf) - For RNN
2. [Recurrent Batch Normalization](https://arxiv.org/pdf/1603.09025.pdf)
3. [Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks](https://arxiv.org/pdf/1602.07868)
4. [Improving LSTM-based Video Description with Linguistic Knowledge Mined from Text](https://arxiv.org/pdf/1604.01729)
5. [On Using Monolingual Corpora in Neural Machine Translation](https://arxiv.org/pdf/1503.03535)
6. [Information-Propagation-Enhanced Neural Machine Translation by Relation Model](https://arxiv.org/pdf/1709.01766.pdf) - Improved decoder, Relational Attention Model
7. [Training RNNs as Fast as CNNs](https://arxiv.org/pdf/1709.02755.pdf)
8. [DiSAN: Directional Self-Attention Network for RNN/CNN-free Language Understanding](https://arxiv.org/pdf/1709.04696.pdf) - Novel attention mechanism
9. [Global-Context Neural Machine Translation through Target-Side Attentive Residual Connections](https://arxiv.org/pdf/1709.04849.pdf)
10. [Attention-based Wav2Text With Feature Transfer Learning](https://arxiv.org/pdf/1709.07814.pdf)
## Datasets
1. [AISHELL-1: an Open-source Mandarin Speech Corpus and a Speech Recognition Baseline](https://arxiv.org/pdf/1709.05522.pdf) - Dataset text
## Related
1. [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) - Word2Vec
2. [RNN Approaches to Text Normalization: A Challenge](https://arxiv.org/pdf/1611.00068.pdf)
3. [Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/pdf/1609.08144.pdf)
